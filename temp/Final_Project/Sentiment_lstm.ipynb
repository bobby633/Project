{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4fd1cc5c-3d33-4178-a761-ef5dfba36d96",
        "_uuid": "7d741102-6897-4ac3-82ec-01e155f4ebed",
        "trusted": true,
        "id": "vDT00LuoDvzg"
      },
      "source": [
        "## Overview \n",
        "\n",
        "This script performs EDA and then preprocesses multiple datasets to train a bidirectional LSTM model which is in turn used to predict the sentiments behind tweets fetched in real time using `tweepy` and classify them as positive negative or neutral.\n",
        "\n",
        "The model is then integrated with streamlit and deployed as a web-app.\n",
        "\n",
        "**Checkout the web-app:** [Sententia](https://share.streamlit.io/kritanjalijain/twitter_sentiment_analysis/main/app.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e9af0762-88ad-444a-98d6-b60ff39fb1f7",
        "_uuid": "e323ac66-f138-4c99-84e7-ffc2d736df95",
        "trusted": true,
        "id": "It-QrenxDvzh"
      },
      "source": [
        "## Installing and importing dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ad397e61-b81d-48bc-bb0c-471762f5620e",
        "_uuid": "f5296f63-a4d8-448f-8a66-4a5a74844b73",
        "trusted": true,
        "id": "NQyYyJuSDvzh"
      },
      "source": [
        "To fetch tweets from twitter, we need to install the tweepy library. We will be using this package to pull tweets on which our model will make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "76f1f3d0-b6fe-44a9-993e-ecfe7f108944",
        "_uuid": "c0e8bf21-9fc1-464b-a11b-6bdf4b0f9fce",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:48:15.342066Z",
          "iopub.status.busy": "2021-06-15T13:48:15.341787Z",
          "iopub.status.idle": "2021-06-15T13:48:22.030557Z",
          "shell.execute_reply": "2021-06-15T13:48:22.028518Z",
          "shell.execute_reply.started": "2021-06-15T13:48:15.342038Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "tFxmFj5ADvzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b2f6e8-3579-47da-bf0f-bd13c11864ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "import os\n",
        "import tweepy as tw #for accessing Twitter API\n",
        "\n",
        "\n",
        "#For Preprocessing\n",
        "import re    # RegEx for removing non-letter characters\n",
        "import nltk  #natural language processing\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "# For Building the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "#For data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.plotting.backend = \"plotly\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "49547739-db4d-4559-b6d2-600ecd225d3e",
        "_uuid": "88e14970-da9b-4403-af5e-c631eede9c19",
        "trusted": true,
        "id": "Se1Eazh-Dvzk"
      },
      "source": [
        "### Cleaning and prepping dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "debf7898-99e6-442b-8102-b8414809d8e3",
        "_uuid": "a89e4673-96aa-4b2f-916c-6c6bce257519",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:48:22.033097Z",
          "iopub.status.busy": "2021-06-15T13:48:22.032452Z",
          "iopub.status.idle": "2021-06-15T13:48:22.675333Z",
          "shell.execute_reply": "2021-06-15T13:48:22.673006Z",
          "shell.execute_reply.started": "2021-06-15T13:48:22.033055Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4HPaCk0Dvzk",
        "outputId": "39e834b3-7b43-4377-d33e-b9f260d3a1c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load Tweet dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Getting Data And Combining Together"
      ],
      "metadata": {
        "id": "r4vd3piLvA0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first one\n",
        "data1 = pd.read_csv(r\"/content/drive/MyDrive/files/Twitter_Data.csv\")\n",
        "#second dataset\n",
        "data2 = pd.read_csv(r'/content/drive/MyDrive/files/apple-twitter-sentiment-texts.csv')\n",
        "data2 = data2.rename(columns={'text': 'clean_text', 'sentiment':'category'})\n",
        "data2['category'] = data2['category'].map({-1: -1.0, 0: 0.0, 1:1.0})\n",
        "#third dataset\n",
        "data3 = pd.read_csv('/content/drive/MyDrive/files/finalSentimentdata2.csv')\n",
        "data3 = data3.rename(columns={'text': 'clean_text', 'sentiment':'category'})\n",
        "data3['category'] = data3['category'].map({'sad': -1.0, 'anger': -1.0, 'fear': -1.0, 'joy':1.0})\n",
        "data3 = data3.drop(['Unnamed: 0'], axis=1)\n",
        "#fouth dataset\n",
        "data4 = pd.read_csv('/content/drive/MyDrive/files/Tweets.csv')\n",
        "data4 = data4.rename(columns={'text': 'clean_text', 'airline_sentiment':'category'})\n",
        "data4['category'] = data4['category'].map({'negative': -1.0, 'neutral': 0.0, 'positive':1.0})\n",
        "data4 = data4[['category','clean_text']]\n",
        "#combine\n",
        "df = pd.concat([data1, data2, data3, data4], ignore_index=True)\n",
        "# drop missing rows\n",
        "df.dropna(axis=0, inplace=True)\n",
        "df['category'] = df['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QbbvLX8MHjgN",
        "outputId": "80e2284a-e04f-4ce9-da2d-313cf05d1890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          clean_text  category\n",
              "0  when modi promised “minimum government maximum...  Negative\n",
              "1  talk all the nonsense and continue all the dra...   Neutral\n",
              "2  what did just say vote for modi  welcome bjp t...  Positive\n",
              "3  asking his supporters prefix chowkidar their n...  Positive\n",
              "4  answer who among these the most powerful world...  Positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8dfb4bb9-696e-4f68-9015-48e6bd82f75a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when modi promised “minimum government maximum...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>talk all the nonsense and continue all the dra...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asking his supporters prefix chowkidar their n...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer who among these the most powerful world...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8dfb4bb9-696e-4f68-9015-48e6bd82f75a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8dfb4bb9-696e-4f68-9015-48e6bd82f75a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8dfb4bb9-696e-4f68-9015-48e6bd82f75a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "de0f402f-09e1-4bc6-bb6a-afc89ee55ad0",
        "_uuid": "f03fa6a3-85de-4c49-8e0f-eebc3abb0f74",
        "trusted": true,
        "id": "6P65c1yPDvzq"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "13bf9b06-9a96-4e39-b3dd-8c24d848c2c2",
        "_uuid": "b452e7a7-8132-4951-a932-d007b3955870",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:48:51.533906Z",
          "iopub.status.busy": "2021-06-15T13:48:51.533574Z",
          "iopub.status.idle": "2021-06-15T13:48:51.553512Z",
          "shell.execute_reply": "2021-06-15T13:48:51.552693Z",
          "shell.execute_reply.started": "2021-06-15T13:48:51.533869Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "rim_FaEYDvzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb952fe-f365-4fa4-c469-f800284f1c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original tweet -> when modi promised “minimum government maximum governance” expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples\n",
            "\n",
            "Processed tweet -> ['modi', 'promis', 'minimum', 'govern', 'maximum', 'govern', 'expect', 'begin', 'difficult', 'job', 'reform', 'state', 'take', 'year', 'get', 'justic', 'state', 'busi', 'exit', 'psu', 'templ']\n"
          ]
        }
      ],
      "source": [
        "def tweet_to_words(tweet):\n",
        "    ''' Convert tweet text into a sequence of words '''\n",
        "    \n",
        "    # convert to lowercase\n",
        "    text = tweet.lower()\n",
        "    # remove non letters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    # tokenize\n",
        "    words = text.split()\n",
        "    # remove stopwords\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
        "    # apply stemming\n",
        "    words = [PorterStemmer().stem(w) for w in words]\n",
        "    # return list\n",
        "    return words\n",
        "\n",
        "print(\"\\nOriginal tweet ->\", df['clean_text'][0])\n",
        "print(\"\\nProcessed tweet ->\", tweet_to_words(df['clean_text'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fdd4e291-fc5e-436a-bae3-fdde3fad279f",
        "_uuid": "792352af-397b-42b1-bd22-97a83548c929",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:48:51.555414Z",
          "iopub.status.busy": "2021-06-15T13:48:51.555058Z",
          "iopub.status.idle": "2021-06-15T13:56:56.429724Z",
          "shell.execute_reply": "2021-06-15T13:56:56.428932Z",
          "shell.execute_reply.started": "2021-06-15T13:48:51.555381Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "yIePgRTQDvzq"
      },
      "outputs": [],
      "source": [
        "# Apply data processing to each tweet\n",
        "X = list(map(tweet_to_words, df['clean_text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f9ff9b12-980d-4a0c-b205-7abba133ba3e",
        "_uuid": "a69f5a61-e641-4d50-be8a-561ffdd13aeb",
        "trusted": true,
        "id": "oYC7kK1sDvzr"
      },
      "source": [
        "### Train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c9e9e40f-3610-4d2f-87d3-a124082b688a",
        "_uuid": "3eb577ca-80df-496b-91e1-ba0aab21fe11",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:56:56.736192Z",
          "iopub.status.busy": "2021-06-15T13:56:56.735579Z",
          "iopub.status.idle": "2021-06-15T13:57:00.775493Z",
          "shell.execute_reply": "2021-06-15T13:57:00.774712Z",
          "shell.execute_reply.started": "2021-06-15T13:56:56.736152Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "xeTKrZoqDvzr"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Encode target labels\n",
        "label = LabelEncoder()\n",
        "Y = label.fit_transform(df['category'])\n",
        "\n",
        "y = pd.get_dummies(df['category'])\n",
        "training_x, testing_x, training_y, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "training_x, X_validation, training_y, y_validation = train_test_split(training_x, training_y, test_size=0.25, random_state=1)\n",
        "v_Size = 5000\n",
        "\n",
        "c_v = CountVectorizer(max_features=v_Size,preprocessor=lambda x: x, tokenizer=lambda x: x) \n",
        "\n",
        "# Fit the training data\n",
        "training_x = c_v.fit_transform(training_x).toarray()\n",
        "\n",
        "# Transform testing data\n",
        "testing_x = c_v.transform(testing_x).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fa0fa00f-3b2e-4bf3-9e56-a3c0e99a5033",
        "_uuid": "e0790578-2f97-45d2-ac1b-eca2dac01166",
        "trusted": true,
        "id": "DJ_OpVFmDvzs"
      },
      "source": [
        "training_xtraining_xtraining_### Tokenizing & Paddingvdfffffdftraining_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ee7d87c0-f428-47d0-92cf-4e3b157d83eb",
        "_uuid": "09fe6021-b3ac-419c-ab3b-3a1f76379a2c",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:57:00.922444Z",
          "iopub.status.busy": "2021-06-15T13:57:00.9221Z",
          "iopub.status.idle": "2021-06-15T13:57:10.389152Z",
          "shell.execute_reply": "2021-06-15T13:57:10.387288Z",
          "shell.execute_reply.started": "2021-06-15T13:57:00.92241Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "BrK2vypTDvzs"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_words = 5000\n",
        "max_len=50\n",
        "\n",
        "def tokenize_pad_sequences(text):\n",
        "    '''\n",
        "    This function tokenize the input text into sequnences of intergers and then\n",
        "    pad each sequence to the same length\n",
        "    '''\n",
        "    # Text tokenization\n",
        "    toke = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
        "    toke.fit_on_texts(text)\n",
        "    # Transforms text to a sequence of integers\n",
        "    X = toke.texts_to_sequences(text)\n",
        "    # Pad sequences to the same length\n",
        "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
        "    # return sequences\n",
        "    return X, toke\n",
        "\n",
        "X, toke = tokenize_pad_sequences(df['clean_text'])\n",
        "\n",
        "\n",
        "\n",
        "# saving\n",
        "with open('/content/drive/MyDrive/files/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(toke, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "# loading\n",
        "with open('/content/drive/MyDrive/files/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "918b178c-362d-4307-950c-67f205eadc68",
        "_uuid": "3428c844-00ef-4add-b27c-794d7aeb5c9b",
        "trusted": true,
        "id": "fut83zHiDvzs"
      },
      "source": [
        "### Saving tokenized data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "53611a76-f3fe-4a8b-a86e-a956ad8dc2b3",
        "_uuid": "0dd5da2a-01b2-4697-a201-736d6a9bd9f1",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:57:10.390829Z",
          "iopub.status.busy": "2021-06-15T13:57:10.390514Z",
          "iopub.status.idle": "2021-06-15T13:57:10.662767Z",
          "shell.execute_reply": "2021-06-15T13:57:10.661979Z",
          "shell.execute_reply.started": "2021-06-15T13:57:10.390795Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "Z1EveG3XDvzs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "17d33b73-9e80-45f3-8dc2-d33353ac7fc7",
        "_uuid": "761a0192-6c66-44b1-988f-530dfc4ac8ce",
        "trusted": true,
        "id": "bBDIefDlDvzs"
      },
      "source": [
        "### Train & Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ab4d79f8-27f4-4384-866a-280442941ef7",
        "_uuid": "8218591d-f858-4e1f-a877-10e74f0885f3",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:57:10.66547Z",
          "iopub.status.busy": "2021-06-15T13:57:10.664774Z",
          "iopub.status.idle": "2021-06-15T13:57:10.905583Z",
          "shell.execute_reply": "2021-06-15T13:57:10.9044Z",
          "shell.execute_reply.started": "2021-06-15T13:57:10.665432Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "ZD55mRZwDvzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240375f0-2984-408c-ddc4-49fd90ee6a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set -> (109397, 50) (109397, 3)\n",
            "Validation Set -> (36466, 50) (36466, 3)\n",
            "Test Set -> (36466, 50) (36466, 3)\n"
          ]
        }
      ],
      "source": [
        "y = pd.get_dummies(df['category'])\n",
        "training_x, testing_x, training_y, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "training_x, X_validation, training_y, y_validation = train_test_split(training_x, training_y, test_size=0.25, random_state=1)\n",
        "print('Train Set ->', training_x.shape, training_y.shape)\n",
        "print('Validation Set ->', X_validation.shape, y_validation.shape)\n",
        "print('Test Set ->', testing_x.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "819dd6c6-7269-4ebd-b130-089311a92c26",
        "_uuid": "f4466e64-4f6a-4868-b430-153d7511a83a",
        "trusted": true,
        "id": "PX0KuC60Dvzt"
      },
      "source": [
        "## Bidirectional LSTM Using NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "34db76fb-fc02-4aa9-86ef-88d30cb5aa01",
        "_uuid": "bb98be32-1c7a-442a-bd28-7bf3a01b6050",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:57:10.919564Z",
          "iopub.status.busy": "2021-06-15T13:57:10.919179Z",
          "iopub.status.idle": "2021-06-15T13:57:13.675214Z",
          "shell.execute_reply": "2021-06-15T13:57:13.674327Z",
          "shell.execute_reply.started": "2021-06-15T13:57:10.919531Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "tT61bwTCDvzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43b1ade-0715-4596-c9e3-df0e08f8d6da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 50, 32)            160000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 50, 32)            3104      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 25, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               16640     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 179,939\n",
            "Trainable params: 179,939\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#!pip install tensorflow\n",
        "#!pip install keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import datasets\n",
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "from tensorflow.keras import losses\n",
        "\n",
        "vocab_size = 5000\n",
        "e_size = 32\n",
        "epochs=20\n",
        "learning_rate = 0.1\n",
        "d_rate = learning_rate / epochs\n",
        "momentum = 0.8\n",
        "\n",
        "Stochastic_Gradient_Descent = SGD(learning_rate=learning_rate, momentum=momentum, decay=d_rate, nesterov=False)\n",
        "# Build model\n",
        "model= Sequential()\n",
        "model.add(Embedding(vocab_size, e_size, input_length=max_len))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "print(model.summary())\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer= Stochastic_Gradient_Descent,metrics=['accuracy', Precision(), Recall()])\n",
        "# Train model\n",
        "batch_size = 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "402d6b63-3bf3-48cf-b590-163908ae68ae",
        "_uuid": "1f2778a3-1caf-4927-aef5-bdef1048457c",
        "execution": {
          "iopub.execute_input": "2021-06-15T13:57:14.165737Z",
          "iopub.status.busy": "2021-06-15T13:57:14.16538Z",
          "iopub.status.idle": "2021-06-15T14:03:03.293306Z",
          "shell.execute_reply": "2021-06-15T14:03:03.2925Z",
          "shell.execute_reply.started": "2021-06-15T13:57:14.165698Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "oRRZRkzqDvzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c818a793-cd31-43d0-d036-2c9573f65353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1710/1710 [==============================] - 244s 138ms/step - loss: 0.9802 - accuracy: 0.5124 - precision: 0.5928 - recall: 0.2524 - val_loss: 0.8881 - val_accuracy: 0.5885 - val_precision: 0.6493 - val_recall: 0.4229\n",
            "Epoch 2/20\n",
            "1710/1710 [==============================] - 196s 115ms/step - loss: 0.8157 - accuracy: 0.6192 - precision: 0.6613 - recall: 0.5125 - val_loss: 0.7052 - val_accuracy: 0.6745 - val_precision: 0.6997 - val_recall: 0.6267\n",
            "Epoch 3/20\n",
            "1710/1710 [==============================] - 191s 112ms/step - loss: 0.6786 - accuracy: 0.6843 - precision: 0.7072 - recall: 0.6460 - val_loss: 0.6472 - val_accuracy: 0.7005 - val_precision: 0.7221 - val_recall: 0.6660\n",
            "Epoch 4/20\n",
            "1710/1710 [==============================] - 153s 89ms/step - loss: 0.6369 - accuracy: 0.7013 - precision: 0.7224 - recall: 0.6679 - val_loss: 0.6204 - val_accuracy: 0.7084 - val_precision: 0.7273 - val_recall: 0.6799\n",
            "Epoch 5/20\n",
            "1710/1710 [==============================] - 165s 97ms/step - loss: 0.6146 - accuracy: 0.7136 - precision: 0.7340 - recall: 0.6785 - val_loss: 0.6127 - val_accuracy: 0.7191 - val_precision: 0.7397 - val_recall: 0.6881\n",
            "Epoch 6/20\n",
            "1710/1710 [==============================] - 169s 99ms/step - loss: 0.5925 - accuracy: 0.7333 - precision: 0.7548 - recall: 0.6954 - val_loss: 0.5715 - val_accuracy: 0.7511 - val_precision: 0.7724 - val_recall: 0.7173\n",
            "Epoch 7/20\n",
            "1710/1710 [==============================] - 142s 83ms/step - loss: 0.5496 - accuracy: 0.7683 - precision: 0.7887 - recall: 0.7359 - val_loss: 0.5213 - val_accuracy: 0.7754 - val_precision: 0.7917 - val_recall: 0.7574\n",
            "Epoch 8/20\n",
            "1710/1710 [==============================] - 137s 80ms/step - loss: 0.4952 - accuracy: 0.8115 - precision: 0.8296 - recall: 0.7851 - val_loss: 0.4650 - val_accuracy: 0.8280 - val_precision: 0.8442 - val_recall: 0.8079\n",
            "Epoch 9/20\n",
            "1710/1710 [==============================] - 137s 80ms/step - loss: 0.4566 - accuracy: 0.8408 - precision: 0.8555 - recall: 0.8190 - val_loss: 0.4308 - val_accuracy: 0.8543 - val_precision: 0.8673 - val_recall: 0.8366\n",
            "Epoch 10/20\n",
            "1710/1710 [==============================] - 112s 65ms/step - loss: 0.4266 - accuracy: 0.8578 - precision: 0.8707 - recall: 0.8393 - val_loss: 0.4131 - val_accuracy: 0.8633 - val_precision: 0.8734 - val_recall: 0.8504\n",
            "Epoch 11/20\n",
            "1710/1710 [==============================] - 131s 77ms/step - loss: 0.4040 - accuracy: 0.8703 - precision: 0.8814 - recall: 0.8549 - val_loss: 0.3891 - val_accuracy: 0.8733 - val_precision: 0.8831 - val_recall: 0.8614\n",
            "Epoch 12/20\n",
            "1710/1710 [==============================] - 124s 73ms/step - loss: 0.3829 - accuracy: 0.8800 - precision: 0.8895 - recall: 0.8671 - val_loss: 0.3840 - val_accuracy: 0.8760 - val_precision: 0.8828 - val_recall: 0.8669\n",
            "Epoch 13/20\n",
            "1710/1710 [==============================] - 146s 85ms/step - loss: 0.3671 - accuracy: 0.8875 - precision: 0.8959 - recall: 0.8758 - val_loss: 0.3600 - val_accuracy: 0.8870 - val_precision: 0.8945 - val_recall: 0.8778\n",
            "Epoch 14/20\n",
            "1710/1710 [==============================] - 136s 80ms/step - loss: 0.3552 - accuracy: 0.8918 - precision: 0.8999 - recall: 0.8811 - val_loss: 0.3583 - val_accuracy: 0.8877 - val_precision: 0.8945 - val_recall: 0.8785\n",
            "Epoch 15/20\n",
            "1710/1710 [==============================] - 138s 81ms/step - loss: 0.3439 - accuracy: 0.8967 - precision: 0.9041 - recall: 0.8865 - val_loss: 0.3461 - val_accuracy: 0.8934 - val_precision: 0.8987 - val_recall: 0.8852\n",
            "Epoch 16/20\n",
            "1710/1710 [==============================] - 134s 79ms/step - loss: 0.3356 - accuracy: 0.8998 - precision: 0.9070 - recall: 0.8907 - val_loss: 0.3350 - val_accuracy: 0.8975 - val_precision: 0.9043 - val_recall: 0.8900\n",
            "Epoch 17/20\n",
            "1710/1710 [==============================] - 131s 77ms/step - loss: 0.3281 - accuracy: 0.9034 - precision: 0.9103 - recall: 0.8946 - val_loss: 0.3335 - val_accuracy: 0.8970 - val_precision: 0.9036 - val_recall: 0.8883\n",
            "Epoch 18/20\n",
            "1710/1710 [==============================] - 132s 77ms/step - loss: 0.3208 - accuracy: 0.9061 - precision: 0.9128 - recall: 0.8977 - val_loss: 0.3305 - val_accuracy: 0.8980 - val_precision: 0.9034 - val_recall: 0.8900\n",
            "Epoch 19/20\n",
            "1710/1710 [==============================] - 126s 74ms/step - loss: 0.3154 - accuracy: 0.9077 - precision: 0.9141 - recall: 0.8994 - val_loss: 0.3243 - val_accuracy: 0.9004 - val_precision: 0.9058 - val_recall: 0.8940\n",
            "Epoch 20/20\n",
            "1710/1710 [==============================] - 144s 84ms/step - loss: 0.3096 - accuracy: 0.9096 - precision: 0.9160 - recall: 0.9015 - val_loss: 0.3167 - val_accuracy: 0.9045 - val_precision: 0.9092 - val_recall: 0.8985\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(training_x, training_y, validation_data=(X_validation, y_validation),batch_size=batch_size, epochs=epochs, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f657ab51-e63e-47b3-8995-f13d79c570cd",
        "_uuid": "844d703e-882a-47c6-a3ba-b4622aa98626",
        "trusted": true,
        "id": "1hK9ekmXDvzt"
      },
      "source": [
        "### Model Accuracy & Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3310bf9e-95ff-4d89-8e64-0f677054bddd",
        "_uuid": "42e7ee95-8192-4410-9c47-9f950a7a870e",
        "execution": {
          "iopub.execute_input": "2021-06-15T14:03:03.296973Z",
          "iopub.status.busy": "2021-06-15T14:03:03.296693Z",
          "iopub.status.idle": "2021-06-15T14:03:07.916302Z",
          "shell.execute_reply": "2021-06-15T14:03:07.915513Z",
          "shell.execute_reply.started": "2021-06-15T14:03:03.296947Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "oTXZPChUDvzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e99d3b-355a-4b19-aa38-d1959809a4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy  : 0.8982\n",
            "Precision : 0.9030\n",
            "Recall    : 0.8925\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model on the test set\n",
        "loss, accuracy, precision, recall = model.evaluate(testing_x, y_test, verbose=0)\n",
        "# Print metrics\n",
        "print('')\n",
        "print('Accuracy  : {:.4f}'.format(accuracy))\n",
        "print('Precision : {:.4f}'.format(precision))\n",
        "print('Recall    : {:.4f}'.format(recall))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "45f9ed85-ee6e-4012-b40d-33455fdb8474",
        "_uuid": "21d1d657-02fc-4db4-8ea1-fee1327b76c1",
        "trusted": true,
        "id": "ydcBCyc4Dvzt"
      },
      "source": [
        "### Model Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2aa7fd7a-c5e5-430a-8f4a-79c331e2486e",
        "_uuid": "f2225ada-4e4c-4b47-82b2-0b43b28dfb40",
        "trusted": true,
        "id": "vMg1PfhTDvzu"
      },
      "source": [
        "### Model save and load for the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "567c602f-0579-4257-8f6b-7b6039fa6729",
        "_uuid": "7005da8f-1988-4e7b-a34d-f1355fe504f7",
        "execution": {
          "iopub.execute_input": "2021-06-15T14:03:11.880904Z",
          "iopub.status.busy": "2021-06-15T14:03:11.880629Z",
          "iopub.status.idle": "2021-06-15T14:03:11.916253Z",
          "shell.execute_reply": "2021-06-15T14:03:11.915216Z",
          "shell.execute_reply.started": "2021-06-15T14:03:11.880862Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "bCtanQL0Dvzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3fa56ed-f800-4fd9-ed6c-e96d7c4cf8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://d4d5cf25-9001-4abc-95a6-9c0d336b10a5/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://d4d5cf25-9001-4abc-95a6-9c0d336b10a5/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f5cb025d3d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f5cb025d8d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "# Save the model architecture & the weights\n",
        "\n",
        "\n",
        "model.save('new_Sentiment_model.h5')\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/files/sentiment_model','wb') as f:\n",
        "    pickle.dump(model,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "80ff55f2-b918-4dea-84a6-d0260c65a0fa",
        "_uuid": "bcfae6c2-a987-443b-a3d4-46722dca3827",
        "execution": {
          "iopub.execute_input": "2021-06-15T14:03:11.917916Z",
          "iopub.status.busy": "2021-06-15T14:03:11.917567Z",
          "iopub.status.idle": "2021-06-15T14:03:12.400898Z",
          "shell.execute_reply": "2021-06-15T14:03:12.400148Z",
          "shell.execute_reply.started": "2021-06-15T14:03:11.917862Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "4cxCqV6wDvzu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "8d992e4b-cc7b-4dd8-af33-28097905bb63"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5000789f1ba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment_model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'''Function to predict sentiment class of the passed text'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sentiment_model'"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "with open('sentiment_model','rb') as f:\n",
        "   model = pickle.load(f)\n",
        "def predict_class(text):\n",
        "    '''Function to predict sentiment class of the passed text'''\n",
        "    \n",
        "    sentiment_classes = ['Negative', 'Neutral', 'Positive']\n",
        "    max_len=50\n",
        "    \n",
        "    # Transforms text to a sequence of integers using a toke object\n",
        "    xt = toke.texts_to_sequences(text)\n",
        "    # Pad sequences to the same length\n",
        "    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
        "    # Do the prediction using the loaded model\n",
        "    yt = model.predict(xt).argmax(axis=1)\n",
        "    # Print the predicted sentiment\n",
        "    print('The predicted sentiment is', sentiment_classes[yt[0]])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Sentiment-lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}